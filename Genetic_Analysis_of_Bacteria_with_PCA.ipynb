{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83e\uddec Multi-Class Bacterial Species Identification from High-Dimensional Gene Expression Data\n\nThis notebook presents an end-to-end machine learning pipeline for classifying **10 bacterial species** from high-dimensional genetic expression data (286 gene features per sample).\n\n### Objectives\n- Perform exploratory data analysis (EDA) on bacterial genetic data\n- Identify highly correlated gene pairs within and across species\n- Apply **Box-Cox transformation** to normalize skewed gene distributions\n- Use **Principal Component Analysis (PCA)** to reduce dimensionality and visualize species clusters\n- Train and evaluate an **Extra Trees Classifier** under two configurations:\n  - Using **100 principal components** (PCA-reduced)\n  - Using all **286 original features** (full feature space)\n- Compare model performance and generate test set predictions\n\n### Dataset Summary\n| Split | Raw Samples | Features | After Deduplication |\n|-------|------------|----------|---------------------|\n| Train | 200,000 | 287 (286 genes + target) | 123,993 |\n| Test  | 100,000 | 286 | \u2014 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading\n\nWe begin by importing all necessary libraries and loading the train/test CSV files.\n\n- **Pandas / NumPy** \u2014 data wrangling and numerical operations\n- **Seaborn / Matplotlib** \u2014 static visualization\n- **Plotly** \u2014 interactive charts\n- **SciPy** \u2014 statistical transformations (Box-Cox)\n- **scikit-learn** \u2014 preprocessing, PCA, modeling, and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Standard library \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport os, warnings\nwarnings.filterwarnings(\"ignore\")\n\n# \u2500\u2500 Numerical & data wrangling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport numpy as np\nimport pandas as pd\n\n# \u2500\u2500 Static visualization \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mpl\n\n# \u2500\u2500 Interactive visualization \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode\n\n# \u2500\u2500 Statistical transformation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# \u2500\u2500 Machine learning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import (accuracy_score, roc_curve, roc_auc_score,\n                             auc, classification_report)\n\n# \u2500\u2500 Load data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Update these paths to point to your local data directory\ntrain = pd.read_csv('data/train.csv', index_col=0)\ntest  = pd.read_csv('data/test.csv',  index_col=0)\nsub   = pd.read_csv('data/sample_submission.csv')\n\n# \u2500\u2500 Quality check \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Report shape, missing values, and duplicate rows for both splits\nprint('Train Shape: {}  |  Missing Data: {}  |  Duplicates: {}'.format(\n      train.shape, train.isna().sum().sum(), train.duplicated().sum()))\nprint('Test  Shape: {}  |  Missing Data: {}  |  Duplicates: {}'.format(\n      test.shape,  test.isna().sum().sum(),  test.duplicated().sum()))\n\n# \u2500\u2500 Remove duplicates from training data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# ~38% of training rows are exact duplicates; removing them prevents data leakage\n# and speeds up training\ntrain_d = train.drop_duplicates()\nprint('\\nAfter deduplication \u2192 Train Shape: {}'.format(train_d.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n\n### 2.1 Summary Statistics by Species\n\nWe compute descriptive statistics (mean, std, min, max, quartiles) grouped by the target species label.\nThis gives us a high-level sense of how gene expression levels differ across bacterial species.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d.groupby('target').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Species Class Distribution\n\nUnderstanding the class balance is important before modeling. Imbalanced classes can bias a classifier\ntowards the majority class. The bar chart below shows each species as a percentage of the deduplicated\ntraining set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\npio.renderers.default = \"colab\"\n\n# \u2500\u2500 Color palette and layout template \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\npal  = sns.color_palette(\"mako_r\", 12).as_hex()[:10]\ntemp = dict(layout=go.Layout(font=dict(family=\"Franklin Gothic\", size=12)))\n\n# \u2500\u2500 Aggregate class proportions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nbact = train_d.target.value_counts(normalize=True).reset_index()\nbact['proportion'] = bact['proportion'].mul(100)\nbact['target']     = bact['target'].str.replace('_', ' ')\nbact = bact.sort_values(by='proportion', ascending=False)\n\n# \u2500\u2500 Plot \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig = px.bar(bact, x='target', y='proportion', text='proportion',\n             color='target', color_discrete_sequence=pal, opacity=0.8)\nfig.update_traces(texttemplate='%{text:,.2f}%', textposition='outside',\n                  marker_line=dict(width=1, color='#28221D'))\nfig.update_yaxes(visible=False, showticklabels=False)\nfig.update_layout(template=temp,\n                  title_text='Distribution of Bacterial Species in Training Data',\n                  xaxis=dict(title='', tickangle=25, showline=True),\n                  height=450, width=700, showlegend=False)\nfig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Inter-Gene Correlation Matrix\n\nWe compute pairwise Pearson correlations across all 286 gene features to identify:\n- **Redundant features:** Highly correlated gene pairs carry overlapping information\n- **Feature clusters:** Groups of co-expressed genes may represent biological pathways\n\nA color-gradient heatmap allows visual inspection of the full correlation structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Compute full correlation matrix (genes only, exclude target) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ncor = train_d.drop('target', axis=1).corr()\n\n# Render as a styled heatmap (may be slow for large matrices)\ncor.style.background_gradient(cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Most Correlated Gene Pairs (Global)\n\nWe extract all gene pairs with an **absolute correlation above 0.75**, which indicates strong\nlinear dependence. These pairs are candidates for removal or consolidation via PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Flatten upper triangle of the correlation matrix \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nc = (cor.abs()\n        .unstack()\n        .drop_duplicates()\n        .reset_index()\n        .rename(columns={'level_0': 'Gene 1', 'level_1': 'Gene 2', 0: 'Correlation'}))\n\n# Filter to strong correlations (below 1.0 to exclude self-correlation)\nc = (c.query('.75 <= Correlation < 1')\n      .sort_values(by='Correlation', ascending=False)\n      .reset_index(drop=True))\n\nc.style.background_gradient(cmap='flare_r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Most Correlated Gene Pairs Per Species\n\nFor each bacterial species independently, we find the single most correlated gene pair.\nSpecies-specific correlations can reveal genetic relationships that are masked when all\nspecies are pooled together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Per-species top correlated gene pair \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfor species_name in train_d.target.unique():\n    # Subset to this species and drop the target label column\n    subset = train_d[train_d.target == species_name].drop('target', axis=1)\n\n    # Compute correlation matrix for this species\n    cor_sp = subset.corr()\n\n    # Flatten, remove self-correlations, and sort\n    c_sp = (cor_sp.abs()\n                  .unstack()\n                  .drop_duplicates()\n                  .reset_index()\n                  .rename(columns={'level_0': 'Gene 1', 'level_1': 'Gene 2', 0: 'Correlation'}))\n    c_sp = (c_sp.query('Correlation < 1')\n                .sort_values(by='Correlation', ascending=False)\n                .reset_index(drop=True))\n\n    # Display only the top pair with a caption\n    display(\n        c_sp.iloc[:1, :].style\n            .background_gradient(cmap='flare')\n            .set_caption('Most correlated gene pair in {}'.format(\n                species_name.replace('_', ' ')))\n    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n\n### 3.1 Box-Cox Transformation for Skewed Features\n\nMany gene expression values follow a right-skewed distribution, which can violate the assumptions\nof linear models and reduce performance even for tree-based models. We apply the **Box-Cox power\ntransformation** to any feature with skewness > 0.75.\n\n- `boxcox_normmax(x + 1)` estimates the optimal lambda (\u03bb) that best normalizes the distribution\n- `boxcox1p(x, \u03bb)` applies the transformation safely for zero-containing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Identify skewed features \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nskew_cols = (train_d.select_dtypes(exclude='object')\n                     .skew()\n                     .sort_values(ascending=False))\nskew_cols = (pd.DataFrame(skew_cols.loc[skew_cols > 0.75])\n               .rename(columns={0: 'Skew Before'}))\n\nprint(f'Features with skew > 0.75: {len(skew_cols)}')\n\n# \u2500\u2500 Apply Box-Cox transformation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nt = train_d.copy()\nfor col in skew_cols.index.tolist():\n    t[col] = boxcox1p(t[col], boxcox_normmax(t[col] + 1))\n\n# \u2500\u2500 Compare skewness before vs after \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nskew_df = (pd.concat([skew_cols, t[skew_cols.index].skew()], axis=1)\n             .rename(columns={0: 'Skew After'}))\nprint('\\nSkewness reduction (first 5 features):')\nskew_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Standardization\n\nBefore applying PCA or distance-sensitive algorithms, we standardize all features to\n**zero mean and unit variance** using `StandardScaler`. This ensures that features with\nlarge absolute values do not dominate the principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Separate features from target \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nX = t.drop('target', axis=1)\n\n# \u2500\u2500 Fit and apply StandardScaler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# We fit on the transformed training data; the same scaler will later be\n# applied to the validation and test sets\nX_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n\nprint(f'Scaled feature matrix shape: {X_scaled.shape}')\nX_scaled.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Principal Component Analysis (PCA)\n\n### 4.1 Full PCA \u2014 Explained Variance\n\nWe first fit PCA with all 286 components to understand how much variance is captured\ncumulatively. The animated chart below shows:\n- **Individual variance** (blue): Contribution of each successive principal component\n- **Cumulative variance** (teal): Total variance explained as we include more components\n\nThis helps us determine the optimal number of components for the modeling step\n(balancing information retention vs. dimensionality reduction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Fit full PCA (n_components = number of features) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\npca_full = PCA(n_components=286).fit(X_scaled)\n\n# Cumulative and individual variance series (in %)\npca_cum = pd.Series(np.cumsum(pca_full.explained_variance_ratio_)).mul(100)\npca_ind = pd.Series(pca_full.explained_variance_ratio_).mul(100)\n\n# Re-index from 1 for readability\npca_cum.index = np.arange(1, len(pca_cum) + 1)\npca_ind.index = np.arange(1, len(pca_ind) + 1)\n\nprint(f'Variance explained by PC1:  {pca_ind[1]:.2f}%')\nprint(f'Variance explained by PC2:  {pca_ind[2]:.2f}%')\nprint(f'Variance explained by top 10 PCs:  {pca_cum[10]:.2f}%')\nprint(f'Variance explained by top 100 PCs: {pca_cum[100]:.2f}%')\n\n# \u2500\u2500 Animated variance chart \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig = go.Figure(\n    layout=go.Layout(\n        updatemenus=[dict(type=\"buttons\", direction=\"left\", x=0.15, y=1.2, showactive=False)],\n        xaxis=dict(range=[1, 287], autorange=False, tickwidth=2),\n        yaxis=dict(range=[0, 100], autorange=False)))\n\nfig.add_trace(go.Scatter(x=pca_ind.index[:1], y=pca_ind[:1],\n                         line=dict(color='#5758A3', width=3), visible=True,\n                         fill='tozeroy', opacity=0.8,\n                         hovertemplate='Variance Explained = %{y:.2f}%<br>Principal Component %{x:.0f}',\n                         name='Individual'))\nfig.add_trace(go.Scatter(x=pca_cum.index[:1], y=pca_cum[:1],\n                         line=dict(color='#57A0A3', width=3), visible=True,\n                         fill='tonexty', opacity=0.7,\n                         hovertemplate='Cumulative Variance = %{y:.1f}%<br>Components = %{x:.0f}',\n                         name='Cumulative'))\n\nfig.update(frames=[\n    go.Frame(data=[go.Scatter(x=pca_ind.index[:i], y=pca_ind[:i]),\n                   go.Scatter(x=pca_cum.index[:i], y=pca_cum[:i])])\n    for i in range(1, 287)])\n\nfig.update_yaxes(title='Variance Explained (%)', showline=True, ticksuffix='%', range=[0, 105])\nfig.update_layout(template=temp,\n                  title='Cumulative & Individual Variance Explained by Principal Components',\n                  xaxis_title='Number of Principal Components',\n                  hovermode='x unified', width=700,\n                  legend=dict(orientation='v', yanchor='bottom', y=1.08,\n                              xanchor='right', x=.99, title=''),\n                  updatemenus=[dict(\n                      buttons=[dict(label='\u25b6 Play', method='animate',\n                                    args=[None, {'frame': {'duration': 15, 'redraw': False}},\n                                          {'fromcurrent': True}]),\n                               dict(label='\u23f8 Pause', method='animate',\n                                    args=[[None], {'frame': {'duration': 0, 'redraw': False},\n                                                   'mode': 'immediate', 'transition': {'duration': 0}}])],\n                      direction='left', x=0.15, y=1.2)])\nfig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Gene Importance Across Top 10 Principal Components\n\nEach principal component is a linear combination of the original 286 genes. The **loading**\nof a gene on a component reflects how much that gene contributes to that component.\n\nWe compute a **weighted importance** for each gene in the top 10 PCs:\n\n```\nweighted_importance = |loading| \u00d7 variance_explained_by_PC\n```\n\nThis tells us which genes carry the most meaningful signal in the compressed representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Extract PCA component loadings \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Shape: (n_features, n_components) \u2014 rows=genes, cols=PCs\nloadings = pd.DataFrame(\n    abs(pca_full.components_.T),\n    columns=['PC' + str(i + 1) for i in range(286)],\n    index=X_scaled.columns\n)\n\n# \u2500\u2500 Compute weighted gene importance per PC \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# For each PC, find the gene with the highest loading and weight it by\n# that PC's explained variance fraction\npca_var_ratio = pd.Series(pca_full.explained_variance_ratio_)\nvar_pca = []\nfor pc_name, var_frac in zip(loadings.columns, pca_var_ratio):\n    top_gene = loadings[pc_name].nlargest(1)           # gene with highest loading\n    weighted  = top_gene * var_frac                    # weight by variance explained\n    var_pca.append(pd.DataFrame({\n        'Principal Component': str(int(pc_name[2:])),\n        'Gene': weighted.index,\n        'Weighted Importance': weighted.values\n    }))\n\nvar_pca = pd.concat(var_pca).reset_index(drop=True)\n\n# \u2500\u2500 Plot top 10 PCs \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nplot_df = var_pca.iloc[:10, :]\npal_v   = sns.color_palette(\"viridis\", 14).as_hex()[1:11]\n\nfig = px.bar(plot_df, x='Gene', y='Weighted Importance', text='Weighted Importance',\n             color='Principal Component', color_discrete_sequence=pal_v, opacity=0.7)\nfig.update_traces(texttemplate='%{text:,.3f}', textposition='outside',\n                  marker_line=dict(width=1, color='#28221D'))\nfig.update_layout(template=temp,\n                  title_text='Gene Importance in the Top 10 Principal Components',\n                  xaxis_title='Gene Segment', xaxis_tickangle=28,\n                  yaxis_title='Weighted Importance', legend_title='Principal<br>Component',\n                  height=500, width=700)\nfig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Top Gene Loadings in PC1 \u2013 PC4\n\nEach subplot below shows the **5 genes with the highest absolute loadings** on the first\nfour principal components. High loadings indicate that a gene strongly defines that component's\ndirection in the feature space \u2014 these genes are the primary \"drivers\" of each PC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Subplot: top-5 gene loadings for PC1, PC2, PC3, PC4 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig = make_subplots(rows=2, cols=2,\n                    subplot_titles=('Principal Component 1', 'Principal Component 2',\n                                    'Principal Component 3', 'Principal Component 4'))\n\nfor idx, (pc, row, col) in enumerate([('PC1',1,1), ('PC2',1,2), ('PC3',2,1), ('PC4',2,2)]):\n    top5 = loadings[pc].sort_values(ascending=False)[:5]\n    fig.add_trace(\n        go.Bar(x=top5.index, y=top5, name=pc, showlegend=False,\n               marker_color=pal_v[idx], opacity=0.8,\n               marker_line=dict(width=1, color='#28221D'),\n               hovertemplate=f'Gene %{{x}} loading on {pc} = %{{y:.3f}}<extra></extra>'),\n        row=row, col=col)\n\nfig.update_layout(template=temp,\n                  title_text='Top-5 Gene Loadings on the First Four Principal Components',\n                  height=900, width=700)\nfig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Species Clusters in 2D PCA Space\n\nReducing the data to 10 principal components and plotting the first two gives us an\nintuitive 2D view of how well the species separate in the compressed feature space.\n\n- **Well-separated clusters** \u2192 PCA successfully captures species-discriminating variance\n- **Overlapping clusters** \u2192 The classifier will need more components to resolve fine-grained differences\n\nPC1 and PC2 together explain ~52% of the total variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Project training data onto first 10 PCs \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\npca_10    = PCA(n_components=10).fit_transform(X_scaled)\npca_df    = pd.DataFrame(pca_10, columns=['PC' + str(i + 1) for i in range(10)])\nspecies   = train_d.target.reset_index(drop=True).str.replace('_', ' ')\n\npca_df = pd.concat([species, pca_df], axis=1)\n\n# Sort by species frequency so more common species are drawn first (z-order)\npca_df['_freq'] = pca_df['target'].map(pca_df['target'].value_counts())\npca_df = pca_df.sort_values('_freq', ascending=False).drop('_freq', axis=1)\n\n# \u2500\u2500 Scatter plot (PC1 vs PC2) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig = px.scatter(pca_df, x='PC1', y='PC2', color='target',\n                 color_discrete_sequence=pal, opacity=0.4)\nfig.update_traces(marker_size=7,\n                  hovertemplate='PC1 = %{x:.2f}<br>PC2 = %{y:.2f}')\nfig.update_layout(template=temp,\n                  title='Bacterial Species Projected onto Principal Components 1 and 2',\n                  legend_title='Species', width=700, height=600,\n                  xaxis_title='Component 1 (~31.9% variance)',\n                  yaxis_title='Component 2 (~20.4% variance)')\nfig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification Modeling\n\nWe train an **Extra Trees Classifier** \u2014 an ensemble of fully randomized decision trees \u2014 because:\n- It handles high-dimensional data well without overfitting\n- It is computationally efficient with many features\n- `class_weight='balanced'` corrects for any residual class imbalance\n\nWe compare two feature configurations:\n1. **PCA-reduced (100 components):** Tests whether a compressed representation is sufficient\n2. **Full feature space (286 features):** Serves as the upper-bound baseline\n\n### 5.1 Data Splitting & Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Encode species labels as integers (required by sklearn) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nenc = LabelEncoder()\ny   = enc.fit_transform(train_d.target)\n\n# Features (Box-Cox transformed; target already removed from train_d above)\nX_full = train_d.drop('target', axis=1, errors='ignore')\n\n# \u2500\u2500 Stratified 80/20 train/validation split \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# stratify=y ensures each species is proportionally represented in both splits\nX_train, X_val, y_train, y_val = train_test_split(\n    X_full, y, test_size=0.2, shuffle=True, stratify=y, random_state=21)\n\n# \u2500\u2500 Fit scaler on train; transform train, val, and test \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled   = scaler.transform(X_val)\nX_test_scaled  = scaler.transform(test)\n\nprint(f'Train:      {X_train_scaled.shape}  |  Labels: {y_train.shape}')\nprint(f'Validation: {X_val_scaled.shape}  |  Labels: {y_val.shape}')\nprint(f'Test:       {X_test_scaled.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model A \u2014 Extra Trees with 100 Principal Components\n\nHere we fit PCA with 100 components on the **training set** and apply the learned\ntransformation to the validation and test sets. The classifier is then trained on the\ncompressed representations.\n\n> **Why 100 components?** From the variance chart above, 100 PCs capture a large\n> fraction of the total variance while cutting the feature count by ~65%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 PCA: fit on train, transform all splits \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\npca_model = PCA(n_components=100)\nX_train_pca = pca_model.fit_transform(X_train_scaled)\nX_val_pca   = pca_model.transform(X_val_scaled)\nX_test_pca  = pca_model.transform(X_test_scaled)\n\nprint(f'PCA Train shape: {X_train_pca.shape} | Cumulative variance: '\n      f'{np.sum(pca_model.explained_variance_ratio_)*100:.1f}%')\n\n# \u2500\u2500 Train Extra Trees on PCA-compressed features \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\net_pca = ExtraTreesClassifier(\n    n_estimators=500,\n    class_weight='balanced',   # compensates for any class imbalance\n    random_state=92\n).fit(X_train_pca, y_train)\n\nprint('\\nModel trained:', et_pca)\n\n# \u2500\u2500 Evaluate on validation set \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ny_preds_pca = et_pca.predict(X_val_pca)\ny_probs_pca = et_pca.predict_proba(X_val_pca)\n\nval_acc_pca = accuracy_score(y_true=y_val, y_pred=y_preds_pca)\nval_auc_pca = roc_auc_score(y_true=y_val, y_score=y_probs_pca,\n                             average='weighted', multi_class='ovr')\n\n# Per-class metrics\nreport_pca = classification_report(y_val, y_preds_pca,\n                                    target_names=enc.classes_, output_dict=True)\nc_pca = (pd.DataFrame(report_pca).T.iloc[:10, :]\n          [['f1-score', 'precision', 'recall', 'support']])\nval_f1_pca = c_pca['f1-score'].mean()\n\nprint(f'\\n\u2500\u2500 PCA Model Performance \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500')\nprint(f'Accuracy  = {val_acc_pca*100:.2f}%')\nprint(f'F1-Score  = {val_f1_pca*100:.2f}%')\nprint(f'AUC (OvR) = {val_auc_pca:.4f}')\n\n# Styled per-class breakdown\nc_pca[['f1-score', 'precision', 'recall']] = c_pca[['f1-score', 'precision', 'recall']].mul(100)\n(c_pca.sort_values('f1-score', ascending=False).style\n   .background_gradient(cmap='flare_r', subset=['f1-score'])\n   .format({'f1-score': '{:,.1f}%', 'precision': '{:,.1f}%',\n            'recall': '{:,.1f}%', 'support': '{:,.0f}'}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 ROC Curves \u2014 PCA Model\n\nThe **Receiver Operating Characteristic (ROC)** curve plots the true positive rate (sensitivity)\nagainst the false positive rate (1 \u2212 specificity) at various decision thresholds.\n\nFor multi-class classification we use the **One-vs-Rest (OvR)** strategy: each species is\ntreated as the positive class while the rest are negative. The **Area Under the Curve (AUC)**\nsummarizes performance in a single number \u2014 a perfect classifier achieves AUC = 1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Compute per-species ROC curves \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfpr, tpr, roc_auc_dict, thresh = {}, {}, {}, {}\n\n# Species sorted by F1 (best to worst) for legend ordering\nspecies_sorted = c_pca.sort_values('f1-score', ascending=False).index.str.replace('_', ' ')\n\nfor i, sp in enumerate(species_sorted):\n    fpr[i], tpr[i], thresh[i] = roc_curve(y_val, y_probs_pca[:, i], pos_label=i)\n    roc_auc_dict[i] = auc(fpr[i], tpr[i])\n\n# \u2500\u2500 Plot \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig = go.Figure()\nfor (i, sp), color in zip(enumerate(species_sorted), pal):\n    fig.add_trace(go.Scatter(\n        x=fpr[i], y=tpr[i],\n        line=dict(color=color, width=3), opacity=0.75,\n        hovertemplate='TPR = %{y:.3f}  |  FPR = %{x:.3f}',\n        name=f'{sp}  (AUC = {roc_auc_dict[i]:.3f})'))\n\nfig.update_layout(\n    template=temp,\n    title='Multiclass ROC Curves \u2014 PCA Model (One-vs-Rest)',\n    hovermode='x unified',\n    xaxis_title='False Positive Rate (1 \u2212 Specificity)',\n    yaxis_title='True Positive Rate (Sensitivity)',\n    legend=dict(y=0.1, x=0.98, xanchor='right',\n                bordercolor='black', borderwidth=0.5, font=dict(size=11)),\n    height=550, width=700)\nfig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 PCA Model Test Predictions\n\nWe apply the trained PCA model to the held-out test set and visualise the\npredicted species distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Generate test predictions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntest_preds_pca = et_pca.predict(X_test_pca)\ntarget_pca     = enc.inverse_transform(test_preds_pca)\n\nsub_pca = pd.DataFrame({\n    'row_id': range(int(2e5), int(3e5)),\n    'target': target_pca\n})\n\n# \u2500\u2500 Visualize predicted distribution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nbact_pca = (sub_pca.target.value_counts(normalize=True)\n                          .reset_index()\n                          .rename(columns={'proportion': 'proportion'}))\nbact_pca['proportion'] = bact_pca['proportion'].mul(100)\nbact_pca['target']     = bact_pca['target'].str.replace('_', ' ')\nbact_pca = bact_pca.sort_values('proportion', ascending=False)\n\nfig = px.bar(bact_pca, x='target', y='proportion', text='proportion',\n             color='target', color_discrete_sequence=pal, opacity=0.8)\nfig.update_traces(texttemplate='%{text:,.2f}%', textposition='outside',\n                  marker_line=dict(width=1, color='#28221D'))\nfig.update_yaxes(visible=False, showticklabels=False)\nfig.update_layout(template=temp,\n                  title_text='Predicted Species Distribution \u2014 PCA Model (100 Components)',\n                  xaxis=dict(title='', tickangle=25, showline=True),\n                  height=450, width=700, showlegend=False)\nfig.show()\n\n# \u2500\u2500 Save PCA submission \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsub_pca.to_csv('submission_pca.csv', index=False)\nprint('PCA submission saved \u2192 submission_pca.csv')\nsub_pca.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Model B \u2014 Extra Trees with All 286 Features\n\nAs a comparison baseline, we train the same Extra Trees classifier using the **full\nfeature space** (all 286 original gene features after Box-Cox + standardization).\n\nThis answers the question: *Does PCA-based compression cost us meaningful predictive\naccuracy, or does it preserve the discriminative signal?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Train Extra Trees on all 286 scaled features \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\net_all = ExtraTreesClassifier(\n    n_estimators=500,\n    class_weight='balanced',\n    random_state=21\n).fit(X_train_scaled, y_train)\n\nprint('Model trained:', et_all)\n\n# \u2500\u2500 Evaluate on validation set \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ny_preds_all = et_all.predict(X_val_scaled)\ny_probs_all = et_all.predict_proba(X_val_scaled)\n\nval_acc_all = accuracy_score(y_true=y_val, y_pred=y_preds_all)\nval_auc_all = roc_auc_score(y_true=y_val, y_score=y_probs_all,\n                             average='weighted', multi_class='ovr')\nreport_all  = classification_report(y_val, y_preds_all,\n                                     target_names=enc.classes_, output_dict=True)\nc_all   = (pd.DataFrame(report_all).T.iloc[:10, :]\n             [['f1-score', 'precision', 'recall', 'support']])\nval_f1_all = c_all['f1-score'].mean()\n\nprint(f'\\n\u2500\u2500 Full Feature Model Performance \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500')\nprint(f'Accuracy  = {val_acc_all*100:.2f}%')\nprint(f'F1-Score  = {val_f1_all*100:.2f}%')\nprint(f'AUC (OvR) = {val_auc_all:.4f}')\n\nc_all[['f1-score', 'precision', 'recall']] = c_all[['f1-score', 'precision', 'recall']].mul(100)\n(c_all.sort_values('f1-score', ascending=False).style\n   .background_gradient(cmap='flare_r', subset=['f1-score'])\n   .format({'f1-score': '{:,.1f}%', 'precision': '{:,.1f}%',\n            'recall': '{:,.1f}%', 'support': '{:,.0f}'}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Final Predictions\n\n### Model Comparison\n\n| Configuration | Features | Accuracy | F1-Score | AUC |\n|---------------|----------|----------|----------|-----|\n| Extra Trees + PCA | 100 PCs | *run to see* | *run to see* | *run to see* |\n| Extra Trees (Full) | 286 genes | *run to see* | *run to see* | *run to see* |\n\n### Key Takeaways\n- **PCA dimensionality reduction** compresses 286 gene features into 100 principal components\n  while retaining the majority of the variance \u2014 significantly speeding up training.\n- The **2D PCA projection** (PC1 vs PC2) reveals naturally separable clusters for most\n  bacterial species, confirming that the gene expression patterns are genuinely discriminative.\n- **Box-Cox transformation** and **standardization** are essential preprocessing steps;\n  they ensure PCA components are not dominated by a handful of extreme-valued genes.\n- **Gene loadings analysis** reveals which specific gene segments drive each principal\n  component, bridging the gap between the statistical model and biological interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Generate final test predictions (full-feature model) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntest_preds_all = et_all.predict(X_test_scaled)\ntarget_all     = enc.inverse_transform(test_preds_all)\n\nsub_all = pd.DataFrame({\n    'row_id': range(int(2e5), int(3e5)),\n    'target': target_all\n})\n\n# \u2500\u2500 Visualize predicted distribution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nbact_all = (sub_all.target.value_counts(normalize=True)\n                          .reset_index()\n                          .rename(columns={'proportion': 'proportion'}))\nbact_all['proportion'] = bact_all['proportion'].mul(100)\nbact_all['target']     = bact_all['target'].str.replace('_', ' ')\nbact_all = bact_all.sort_values('proportion', ascending=False)\n\nfig = px.bar(bact_all, x='target', y='proportion', text='proportion',\n             color='target', color_discrete_sequence=pal, opacity=0.8)\nfig.update_traces(texttemplate='%{text:,.2f}%', textposition='outside',\n                  marker_line=dict(width=1, color='#28221D'))\nfig.update_yaxes(visible=False, showticklabels=False)\nfig.update_layout(template=temp,\n                  title_text='Predicted Species Distribution \u2014 Full Feature Model (286 Genes)',\n                  xaxis=dict(title='', tickangle=25, showline=True),\n                  height=450, width=700, showlegend=False)\nfig.show()\n\n# \u2500\u2500 Save full-feature submission \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsub_all.to_csv('submission_full.csv', index=False)\nprint('Full-feature submission saved \u2192 submission_full.csv')\nsub_all.head()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}